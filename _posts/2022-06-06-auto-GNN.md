---
layout: post
title: Auto-GNN - Neural architecture search of graph neural networks
subtitle : K. Zhou et al. / 2022 
tags: [NAS, performance prediction, GNN]
author: Hyunju Kim
comments : False
---

# auto-GNN

[Abstract]

 {문제점}

- 적합한 GNN architecture을 고르기 위해 사람의 작업이 필요함
    
    → GNN 성능이 graph convolutional components (aggregate function, hidden dimension) 에 영향을 많이 받기 때문
    
- NAS는 효울적인 deep architecture을 발견함
    
    → GNN에는 부적합했음
    
    1. NAS search space와 GNN search space가 다름
    2. representation learning capacity가 architecture motification에 따라 달라짐
    3. parameter sharing 같은 NAS technique들이 GNN에 unstable함
    

{해결방법}

⇒ predefined search space에서 optimal GNN architecture을 찾는 **AGNN frame work** 개발

- homogeneous architecture는 parameter share 가능

⇒ 현재 handcraft 모델과 전통적인 search methods보다 성능이 뛰어남

[1. Introduction]

- Graph attention networks: protein-protein interactions
- GraphSage: sensitive to hidden dimension
- NAS: find optimal neural architecture in the **predefined search space** performance on a **given task**
    
    ⇒ outperformed: image classification, semantic image segmentation, image generation
    
    ⇒ **(paper’s) TASK:** node classification
    
- problem of GNN on NAS
    1. difference of search space
        
        → CNN convolution operation: kernel size
        
        → message-passing based graph convolution: sequence of action (aggregation et. al.)
        
    2. traditional controller is inefficient to discover GNN architecture
        
        → representation learning capacity of GNN architecture varies with architecture modification
        
    3. widely-used techniques in NAS (ex. parameter sharing) not suitable to GNN architecture
- solution: automated neural architecture search problem 제안
    1. define search space of GNN architecture and explore
    2. constrain parameter sharing among heterogeneous GNN architectures

[2. Problem statement]

![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled.png)

- F: search space (based on graph convlutions)
- D: training set, validation set
- M: evaluation metric (f1 score, accuracy for node classification)
- f*: optimal GNN structure
- f, L: loss function
- theta*: parameter learned

 

 {AGNN}

![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%201.png)

- update each RNN encoder independently to learn the affect of specific action class to model performance.
- weight only shared with homogeneous architectures
- update with offspring vs reuse the old one

[3. Search space]

- layers of message-passing based graph convolutions으로 구성
- k-th layer:
    
    ![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%202.png)
    
    - W: trainable matrix
    - a: attention coefficient obtained from the additional attention layer

⇒ search space를 6개의 action class로 decompose

1. hidden dimension: x(k-1) → W(k) -(map)→ d-dimensional space {4, 8, …, 256}
2. attention function: focus on relevant neighbors to improve the representive learning of node embedding
3. attention head: multi-head-beneficial {1, 2, …, 16}
4. aggregate function: {summation, mean, maxpooling}
5. combine function: Wx, h - node, neighbors differentiable function can enhance node representation learning. {Identity, MLP-2layer perceptron with 128 dimensions}
6. activation function: {Sigmoid, Tanh, ReLU, Linear, Softplus, LeakyReLU, ReLU6, ELU}

![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%203.png)

→ GNN architecture: string of length 6n (n: number of graph convolutional layers)

→ each layer: cardinality of six action classes = 7*7*6*3*2*8=14112

[4. Reinforce Conservative Controller]

- traditinal RL-based NAS
    - RNN applied to specify variable-length NA
    - generate a new candidate architecture
    - validating new architecture → scalar reward

⇒ proposed “RCNAS” consists of

1. conservative explorer -exploitation
2. architecture modifier
3. reinforcement learning trainer

[4.1 Conservative Explorer]

- maintain best NA found so far
- offspring architecture outperforma its parent → update best NA
- OW, reuse to generate the next offspring architecture
- multiple starting points → avoid trapping in local min

[4.2 Guidied Architecture Modifier]

- modify best architecture via selecting and mutating

  {step}

1. For each class, an independent RNN encoder decides a sequence of new actions
    
    1) subarchitecture of length 5n is generated by removing n actions of concerned class
    
    2) subarchitecture string -(input)→ RNN encoder
    
    3) RNN encoder outputs the candidate action
    
2. An action guider receives the decision entropy and selects the action classes to be modified
    - Pi: output of softmax classifier of hidden state hi
    - decision entropy of class c:
    
    ![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%204.png)
    
3. An architecture modification generates the final offspring architecture

[4.3 Reinforcement Learning Trainer]

- use REINFORCE rule of policy gradient to update parameters $\theta_c$ for RNN encoder of class $c\in C$.
- Update rule:
    
    ![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%205.png)
    
    where a_i: action of class c, Rc: reward for taking c 
    

[5. Constrained Parameter Sharing]

- two NA are heterogeneous = have different shapes of trainable weight or output statistics (sigmoid: [0,1], linear: [-inf, +inf]
- three constraints (ancestor and offspring architecture)
    1. graph convolutional layer의 input, output tensors의 same shape을 가짐
    2. same attention function and activation function
    3. BN과 SC의 parameter은 공유되지 않음 (train with a few epochs)
    
    ![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%206.png)
    

[6. Experiment]

- find optimal GNN architecture given node classification task
    
    Q1: AGNN과 SOTA handcrafted architecture 비교
    
    Q2: RCNAS controller과 다른 search method 비교
    
    Q3: constrained strategy가 weight을 효율적으로 공유하는지
    
    Q4: architecture modification 스케일이 RCNAS controller의 search efficiency에 영향을 미치는지
    

[6.1 Datasets]

![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%207.png)

- consider both transductive and inductive learning settings for the node classification task
    - transductive: training process에서 validiation과 test set의 node label 제외하고 complete graph structure과 node feature 사용 가능
    - inductive learning: (같은 세팅에서) graph structure과 node feature에 대한 정보 없음

[6.2 Baseline Methods]

- include Chebyshev, GCN, GraphSAGE, GAT, LGCN
- NAS approach: based on reinforcement learning and random search

[Experiment]

![Untitled]({{ site.baseurl }}/assets/img/autoGNN//Untitled%208.png)

(이하 생략)