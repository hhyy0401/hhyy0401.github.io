---
layout: post
title: Policy GNN - Aggregation Optimization for Graph Neural Networks
subtitle : K. Lai et al. / 2020 / KDD
tags: [NAS, performance prediction, GNN]
author: Hyunju Kim
comments : False
---



[Abstract]

- propose to explicitly sample diverse iterations of aggregation for different nodes to boost the performance of GNNs
- propose Policy-GNN, a meta-policy framework that models the sampling procedure and message passing of GNNs into a combined learning process.
- Policy-GNN adaptively determines the number of aggregations for each node and uses meta-policy (trained with RL)

[1. Introduction]

- key techniques behind applications: graph representation learning (extract information into low-dim vector representations)
- previous studies of advancing GNNs
    1. sampling strategies: batch sampling, importance sampling
        
        → cause information loss, sub-optimal performance
        
    2. novel message passing functions like pooling layers, multi-head attentions
        
        → studied oversmoothing problem
        
        - solution: skip-connection. (sub-optimal)
- advocate explicitly sampling diverse iterations of aggregation for different nodes
    
    ⇒ different nodes require different iters of aggregation to fully capture the structural information
    
    ![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled.png)
    
    - nontrival
        1. real-world graphs are complex (suitable iters of aggregation for each node?)
        2. how to train?
    
    ⇒ propose Policy-GNN: meta-policy framework to model the complex aggregation strategy
    
    1. formulate graph representation learning as MDP
        - MDP iteratively samples the number of hops of current nodes and neighboring nodes with a meta-policy and trains GNNs by aggregating the information of nodes within the sampled hops
        - integrates the sampling procedures and message passing into a combined learning process
    2. showcase an instance of our framework by implementing it with deep Q-learning (DQN) and GCN
- contributions
    1. introduce diverse aggregation → boost performance of GNN
    2. formulate GNN training problem → MDP, propose meta-policy framework that uses RL to adaptively optimize the aggregation strategy
    3. develop a practical instance of our framework based on DQN and GCN: outperformances SOTA
        
        → code: [https://github.com/datamllab/Policy-GNN](https://github.com/datamllab/Policy-GNN)
        

[2. Preliminaries]

[2.1 Problem definition]

- GRL: embed graph into low-dimensional vector spaces
- goal: given an arbitrary graph neural network algorithm, jointly learn a meta-policy $\tilde{\pi}: v\in V \rightarrow \text{[number of iterations of aggregation]}$ with the GNN

[2.2 Learning Graph Representations with Deep Neural Networks]

- 3 steps of learning GNN
    1. initialization
    2. neighborhood detection
    3. information aggregation
        - several functions
    
    {example}
    
    1. GCN
        
        ![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%201.png)
        
    2. GAT
        - substitue $\tilde{A}$ with self-attention scores(importance of aggregation from u to v)
    
    ⇒ GCN is expesnve when G becomes larger
    
    1. Local sampling (GraphSAGE)
        - aggregates a fixed number of directed neighbors
    2. Global sampling (FastGCN)
        - introduce an importance sampling among all of nodes
        - capture global information of the graph
    3. Policy GNN
        1. local sampling → MDP
        2. global information through training a meta-policy to make decisions on k-hop aggregations
        
        ⇒ first build upon GCN (but can exten to others)
        

[2.3 Sequential Decision Modeling with Markov Decision Process]

- M: MDP, quintuple (S, A, P_T, R, $\gamma$)
    - S: finite set of states
    - A: finite set of actions
    - P_T: S*A*S → R+ : state transition probability function
    - R: immediate reward function
    - r: discount factor
    
    ⇒ aim: find a policy $\pi: S\rightarrow A$ to maximize $E_{\pi}[\sum_{t=0}^{\infty} \gamma^t r_t]$
    

[2.4 Solving Markov Decision Process with Deep Reinforcement Learning]

- Deep reinforcement learning: a family of algorithms that solve that MDP with deep neural networks
- consider model-free deep reinforcement learning
- DQN uses DNN to approximate state-action values:
    
    ![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%202.png)
    
    {two techniques}
    
    1. replay buffer to reuse past experiences
    2. separate target network that is periodically updated

⇒ We employ DQN(deep reinforcement learning) to solve The MDP

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%203.png)

[3. Methodology]

- two components of framework
    1. meta-policy module: node attributes → iterations
    2. GNN: exploits meta-policy to learn the graph representations
    
    ![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%204.png)
    
- combining two modules, formulate the graph representation learning as MDP
    - meta-policy treats node attributes as state, map state into action
    - sample next state from the k-hop neighbors, where k is the output of Meta-policy(action)
    - GNN module select a pre-built k-layer GNN architeccture to learn the node representations and obtains a reward signal for updating Meta-policy

[3.1 Aggregation Optimization with Deep Reinforcement Learning]

- how to define S, A, R?

 {proposed aggregation process}

1. select a start node, obtain its attributes as the current state $s_t$.
2. generate an action from $a_t$ from $\pi(s_t)$ (number of hops for current node)
3. sample the next node from $a_t$-hop neighborhood and obtain its attributes as the next state $s_{t+1}$.

⇒ To optimize above MDP, we use deep reinforcement learning algorithms (Q-learning)

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%205.png)

- rightmost: baseline for each timestep t
- M: evaluation metric of a specific task
- b: hyperparameter to define the window size
- $\lambda$: hyperparameter to decide the strength of reward signal
- M(s, a): performance of the task

⇒ With this equation, we train Q function optimizing DQN equation above. Also, apply epsilon-greedy policy to obtain our policy function $\tilde{\pi}$.

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%206.png)

[3.2 Graph representation learning with Meta-policy]

- recent efforts on skip-connection: “implicitly” learns an aggregation strategy
- proposed framework: “explicitly” aggregates the information, also include skip-connection
    - aggregates $a_t$ layers for each node attributes $s_t$ at timestep t.
        
        ![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%207.png)
        

[3.3 Accelerating Policy-GNN with Buffer Mechanism and Parameter Sharing]

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%208.png)

- main problem of policy-GNN: training efficiency since reconstructing GNNs at every timestep is time-consuming, #parameters may be large
    
    {solution}
    
    - parameter sharing
    - buffer mechanism for GRL
        - for each timestep, store [a batch of nodes’ attributes and a batch of actions] into the action buffer and check if the buffer reached the batch size
        - if full, construct GNN with selected layers and train GNN
        - after training, clear buffer

[4. Experiment]

Q1. Policy-GNN vs SOTA GRL

Q2. distribution of the number of layers?

Q3. iteration is important?

[4.1 Datasets]

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%209.png)

[4.2 Baselines]

- Network Embedding: DeepWalk, Node2vec (both via random walk sampling)
- Static-GNNs: Chebyshev, GCN, GraphSAGE, FastGCN, GAT, LGCN
- NAS-GNN: GraphNAS, AGNN
- Random Policy (make decision with RL)

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%2010.png)

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%2011.png)

![Untitled]({{ site.baseurl }}/assets/img/pGNN/Untitled%2012.png)

[5. Conclusion]

- policy-GNN outperforms SOTA performance
- limitations
    - training efficiency
    - apply different RL algorithms and advanced aggregation functions
    - combine meta-policy with NAS to jointly optimize the neural architecture and the aggregation strategy