---
title: Policy GNN - Aggregation Optimization for Graph Neural Networks
categories:
- General
---


1. GCN

![picture 2](../../images/b526efd15abb0fd18d064f7c1a0a01279e64172d633659d000ac1175d4e09eb9.png)  

2. GAT

- substitue $\tilde{A}$ with self-attention scores(importance of aggregation from u to v)

⇒ GCN is expesnve when G becomes larger

1. Local sampling (GraphSAGE)
- aggregates a fixed number of directed neighbors
2. Global sampling (FastGCN)
- introduce an importance sampling among all of nodes
- capture global information of the graph
3. Policy GNN
1. local sampling → MDP
2. global information through training a meta-policy to make decisions on k-hop aggregations

⇒ first build upon GCN (but can exten to others)


[2.3 Sequential Decision Modeling with Markov Decision Process]

- M: MDP, quintuple (S, A, P_T, R, $\gamma$)
- S: finite set of states
- A: finite set of actions
- P_T: S*A*S → R+ : state transition probability function
- R: immediate reward function
- r: discount factor

⇒ aim: find a policy $\pi: S\rightarrow A$ to maximize $E_{\pi}[\sum_{t=0}^{\infty} \gamma^t r_t]$


[5. Conclusion]

- policy-GNN outperforms SOTA performance
- limitations
- training efficiency
- apply different RL algorithms and advanced aggregation functions
- combine meta-policy with NAS to jointly optimize the neural architecture and the aggregation strategy