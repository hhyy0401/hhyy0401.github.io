---
layout: post
title: Graph differentiable architecture search with structure learning
subtitle : Y. Qin et al. / 2021 / NeurIPS
tags: [NAS, performance prediction, GNN]
author: Hyunju Kim
comments : False
---


[abstract]

- existing works: fails to answer “HOW Nas is able to select the desired GNN architectures”
    
    ⇒ firstly answer to this
    
- conduct theoretical analysis and measurement study
- discover gradient based NAS: suboptimal (noises hidden in graph)
- propose GASSO: SOTA

1. Introduction
- Paper focuses on gradient based architecture search approach
- DARTS on GNN
    - evaluate the usefulness of information hidden in node features and graph structure w.r.t target task → automatically select appropriate operations
    - deteriorated by noises inside the graphs → suboptimal architectures
- 해결법: GASSO
    - adaptive noise reduction
- contributions
    1. 이론적으로 어떻게 graph neural architecture search가 desired GNN architecture을 선택하는지 최초로 탐색
    2. 실험으로 measurement study 설계
        1. gradient based graph architecture search는 graph의 유용한 정보에 기반하여 operation 선택 가능
        2. graph feature의 noise는 architecture serach performance 감소시킴
    3. GASSO 제안
        - search optimal architectures through a joint optimization scheme
        - SOTA

![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled.png)

[2. Related Work]

[2.1 GNN]

- GCN: noise와 attack에 취약 ⇒ How to denoise and defend graph adversarial attacks?

[2.2 (Graph) NAS]

- relationship between
    - NAS methods
    - GNN denoising ability
    
    has not been noticed
    

[3. Exploring Architecture Search for Graph]

[3.1 Preliminaries: Differentiable Architecture Search]

- use DARTS
    - represent NA as DAG
        - input: source
        - output: sink
        - edge: operation (GCN layer, linear layer)
    - search which operation should be selected, how nodes in DAG should connected
    - two phases
        1. searching phase
            - super network constructed
            - all possible operations at all possible positions are contained in it
            - uses GD based methods
                
                ![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%201.png)
                
        2. evaluation phase
            - new network based on the designed architecture  is constructed
            

[3.2 Theoretical Analysis of Architecture Parameters]

- DARTS behavior on graph task
    1. how does graph NAS select its desired architectures?
    2. how optimal are the architectures selected by GNAS?
- Answer of 1 above: focus on mixed operation in DARTS
    
    ![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%202.png)
    
    → DARTS judges different candidate operations by a score (weighted sum of logits) 
    
    ex) If $y_k=0$, weight is positive, data with bad prediction have larger weights
    
    → DARTS는 hard data에서 prediction을 correct 할 수 있는 operations 선호
    

[3.3 Synthetic Graph Experiment Setting]

- 600 nodes with 10 types, 60 nodes per type which follows Gaussian $\mathcal{N}(\mu_i, \sigma_1)$ where $\mu_i$ follows $\mathcal{N}(0, \sigma_2)$.
    - $\beta=\sigma_2/\sigma_1$: difficulty of classifying nodes by their features
- $p_1$: generating edge between two intra-group nodes, $p_2$: that of inter-group nodes
    - $\delta=p_1/p_2$: difficulty of classifying nodes by message passing scheme
- only consider one target node, connected with only two classes of nodes (one for the node)

![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%203.png)

- when $\beta$ increases, $\abs{D}$ increases, prob. of choosing linear increases
- when $\delta$ increases, prob. of choosing GCN increases

[3.4 Operation Selection]

- how $\delta$ and $\beta$ influnce DARTS?
    
    ⇒ generate synthetic graph by controlling variables
    
    ![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%204.png)
    
- for different graphs, DARTS suggests using different operations
    
    ex1) For more structure information (large $\delta$) and less feature information (small $\beta$), DARTS tends to select typical message passing operatins like GIN.
    
    ex2) In the opposite cases, DARTS likely to select linear, skip-connect to prevent message passing
    
- GNN: information extractor for node classification
- operations in the search space: different ability to extract information from node feature and graph structure
    - GCN: extract from graph structure, reduce of the node’s feature
    - linear: only uses the feature and ignore graph structure
    
    ⇒ DARTS: find optimal information extractor to help correct the prediction of the super network
    
    ![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%205.png)
    
    ⇒ two experiment results are similar: DARTS judges the relative amount of two types of information
    
- DARTS may have denoising ability, but unstable → explore how accurate DARTS selects operations?

[3.5 Accuracy of Operation Selection]

- DARTS may not be able to accurately select the optimal architecture with the original graph
- DARTS may have overconfidence and cause over-fitting

⇒ DARTS cannot design the optimal architecture due to certain noise in the original graph → have to denoise graph structure

[4. Joint optimization of graph architecture and structure]

[4.1 Differentiable graph structure]

- noise
    
    ex) reasons of citation differ
    
- apply differentiable graph structure
    - restrict weight is in (0, 1) during message passing
    - $G_p=normalize(sigmoid(G))$where G is the parameter
        - normalize function: includes adding self-loops(fixed). only changes weight of normal edges
        - benefit
            1. distinguish helpfulness of edges and aggregate more useful information
            2. use gradient based methoes to optimize the structure
            
            ⇒ differentiable graph structure fits most of GNNS by replacing adjacency matrix → $G_p$.
            

[4.2 Learning by Hidden Feature Smoothness Constraint]

- assumption: first-order proximity
- more links between intra-group nodes and fewer links between inter-group nodes → high node classification accuracy
- We use hidden feature H for node classification
- To minimize connections betwen inter-group nodes, use hidden feature smoothness as a regularizer
    
    ![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%206.png)
    
    - $\lambda$: hyperparameter to contol the contribution of the hidden feature smoothness
    - $G_{o, ij}$: initialization

[4.3 Training Procedure]

![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%207.png)

- W, A, G: need to be optimized
- 하나씩 update

[5. Experiments]

![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%208.png)

![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%209.png)

[5.2 Abalation studies]

- GASSO first parts of variants - graph structure learning model
    - V1: remove structure learing part → pure DARTS
    - V2: hidden feature smoothness → original feature smoothness
    - V3: hidden feature smoothness → one-hot predicted label smoothness
    - V4: directly calculate edge weight based on similarities between nodes
- GASSO second parts of variants - neural architecture search model
    - V5: do not update the neural architecture
    - V6: fix GCN as the architecture
    - V7: fix GAT as the architecture
- GASSO third part- optimization scheme
    - V8: split training NA and graph structure
        - train the graph structure for first half epoch
        - train NA for the second half
    - V9: order changed with V8
    

[5.3 Denoising Analysis]

![Untitled]({{ site.baseurl }}/assets/img/diffGNN/Untitled%2010.png)

[6. Conclusion]

- gradient based graph architecture search can select proper operations according to the usefulness of information inside the graph, and may suffer from noises in graph features and structures.
- propose GASSO