---
layout: post
title:  Neural Architecture Performance Prediction Using Graph Neural Networks
subtitle : J. Lukasik et al. / 2020 / DAGM GCPR
tags: [NAS, performance prediction, GNN]
author: Hyunju Kim
comments : False
---


[Abstract]

- propose a surrogate model for neural architecture performance prediction built upon GNN

[1. Introduction]

 {problem}

- NAS-Bench-101: enables to experiment with classical data-based methods
    - high computational cost
    - all architectures have to be evaluated (practical restrictions)
    
    ⇒ calls for accurate surrogate models that enables to extrapolate (i.e. zero shot prediction)
    

{proposed}

1. takle the task of learning to predict the accuracy of convolutional nerual networks in a supervised way
2. evaluate our proposed model on two different zero shot prediction scenarios and show its ability

- NAs for CV: DAGs로 표현 가능 ⇒ surrogate model on GNN은 강력함 ⇒ 노드를 continuous spaces에 embed 가능
- DGMG: capture the structure of graph. NAs를 feature representation에 embed 가능

[2.3 Performance Predictor for Neural Architectures]

- limited

[2.4 GNN]

- discrete nature of graph → can’t trivially be optimized in differentiable learning methods that act on continuous spaces
- breakthroughs: CV, NLP, recommender systems, chemistry and others
- continuous method: GNN -handle graphs chactererizing neural architectures from the NAS-bench 101

[3. The Graph Encoder]

- GNN-based model: encode discrete graph space of NAS-Bench-101 → continuous vector space
- single GNN iteration:  2-step procedure
    1. node: send messages to neighbors
    2. node: aggregates incoming messages
    

[3.1 Node-level propagation]

- Progating information = iterative message-passing process
- For NAS-Bench-101 graphs, add reverse message module (3), (4)

![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled.png)

- aggregation function: sum
- To increase capacity of model
    1. apply multiple rounds of propagation
    2. use different oset of parameters
    
    ![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled%201.png)
    

[3.2 Graph-Level Aggregation]

- After final round of message-passing: node embeddings h -(aggregate)→ single graph embeeding $h_G$.

[4. Model Details]

[4.1 Message]

- message module $M^{(t)}$

![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled%202.png)

[4.2 Update]

- update module $U^{(t)}$ in a single GRU cell

![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled%203.png)

[4.3 Aggregation]

- two rounds of propagation → aggregate the node embeddings into a single graph embedding
    1. linear layer transforms the node embeddings to the required graph embedding $d_g$.
    2. another linear layer combined with a sigmoid handles each node’s fraction in the graph embedding
    
    ![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled%204.png)
    

[5. The NAS-Bench-101 Dataset]

- CIFAR-10-classification set

[6. Experiments]

1. evaluate the performance prediction ability of the proposed GNN in the traditional supervised setting
2. conduct zero shot prediction experiments in order to show the performance of the proposed model for unseen graph structures during training
3. compare result with Tang et al.

![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled%205.png)

![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled%206.png)

- task of predicting the validation accuracy of structurally unknown graph types, i.e. zero shot prediction. (1. train on graph of length 2~7 and test on 7 / 2. train on graph of length 2~6 and test on 6)

![Untitled]({{ site.baseurl }}/assets/img/NAPGNN/Untitled%207.png)

[7. Conclusion]

- The GNN encoder is a powerful tool regarding supervised performance prediction and also especially in the zero-shot setup.